{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Tokenization doesn't have to be slow !\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Before going deep into any Machine Learning or Deep Learning Natural Language Processing models, every practitioner\n",
    "should find a way to map raw input strings to a representation understandable by a trainable model.\n",
    "\n",
    "One very simple approach would be to split inputs over every space and assign an identifier to each word. This approach\n",
    "would look similar to the code below in python\n",
    "\n",
    "```python\n",
    "s = \"very long corpus...\"\n",
    "words = s.split(\" \")  # Split over space\n",
    "vocabulary = dict(enumerate(set(words)))  # Map storing the word to it's corresponding id\n",
    "```\n",
    "\n",
    "This approach might work well if your vocabulary remains small as it would store every word (or **token**) present in your original\n",
    "input. Moreover, word variations like \"cat\" and \"cats\" would not share the same identifiers even if their meaning is \n",
    "quite close.\n",
    "\n",
    "![tokenization_simple](https://cdn.analyticsvidhya.com/wp-content/uploads/2019/11/tokenization.png)\n",
    "\n",
    "### Subtoken Tokenization\n",
    "\n",
    "To overcome the issues described above, recent works have been done on tokenization, leveraging \"subtoken\" tokenization.\n",
    "**Subtokens** extends the previous splitting strategy to furthermore explode a word into grammatically logicial sub-components learned\n",
    "from the data.\n",
    "\n",
    "Taking our previous example of the words __cat__ and __cats__, a sub-tokenization of the word __cats__ would be [cat, ##s]. Where the prefix _\"##\"_ indicates a subtoken of the initial input. \n",
    "Such training algorithms might extract sub-tokens such as _\"##ing\"_, _\"##ed\"_ over English corpus.\n",
    "\n",
    "As you might think of, this kind of sub-tokens construction leveraging compositions of _\"pieces\"_ overall reduces the size\n",
    "of the vocabulary you have to carry to train a Machine Learning model. On the other side, as one token might be exploded\n",
    "into multiple subtokens, the input of your model might increase and become an issue on model with non-linear complexity over the input sequence's length. \n",
    " \n",
    "![subtokenization](https://nlp.fast.ai/images/multifit_vocabularies.png)\n",
    " \n",
    "Among all the tokenization algorithms, we can highlight a few subtokens algorithms used in Transformers-based SoTA models : \n",
    "\n",
    "- [Byte Pair Encoding (BPE) - Neural Machine Translation of Rare Words with Subword Units (Sennrich et al., 2015)](https://arxiv.org/abs/1508.07909)\n",
    "- [Word Piece - Japanese and Korean voice search (Schuster, M., and Nakajima, K., 2015)](https://research.google/pubs/pub37842/)\n",
    "- [Unigram Language Model - Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates (Kudo, T., 2018)](https://arxiv.org/abs/1804.10959)\n",
    "- [Sentence Piece - A simple and language independent subword tokenizer and detokenizer for Neural Text Processing (Taku Kudo and John Richardson, 2018)](https://arxiv.org/abs/1808.06226)\n",
    "\n",
    "Going through all of them is out of the scope of this notebook, so we will just highlight how you can use them.\n",
    "\n",
    "### @huggingface/tokenizers library \n",
    "Along with the transformers library, we @huggingface provide a blazing fast tokenization library\n",
    "able to train, tokenize and decode dozens of Gb/s of text on a common multi-core machine.\n",
    "\n",
    "The library is written in Rust allowing us to take full advantage of multi-core parallel computations in a native and memory-aware way, on-top of which \n",
    "we provide bindings for Python and NodeJS (more bindings may be added in the future). \n",
    "\n",
    "We designed the library so that it provides all the required blocks to create end-to-end tokenizers in an interchangeable way. In that sense, we provide\n",
    "these various components: \n",
    "\n",
    "- **Normalizer**: Executes all the initial transformations over the initial input string. For example when you need to\n",
    "lowercase some text, maybe strip it, or even apply one of the common unicode normalization process, you will add a Normalizer. \n",
    "- **PreTokenizer**: In charge of splitting the initial input string. That's the component that decides where and how to\n",
    "pre-segment the origin string. The simplest example would be like we saw before, to simply split on spaces.\n",
    "- **Model**: Handles all the sub-token discovery and generation, this part is trainable and really dependant\n",
    " of your input data.\n",
    "- **Post-Processor**: Provides advanced construction features to be compatible with some of the Transformers-based SoTA\n",
    "models. For instance, for BERT it would wrap the tokenized sentence around [CLS] and [SEP] tokens.\n",
    "- **Decoder**: In charge of mapping back a tokenized input to the original string. The decoder is usually chosen according\n",
    "to the `PreTokenizer` we used previously.\n",
    "- **Trainer**: Provides training capabilities to each model.\n",
    "\n",
    "For each of the components above we provide multiple implementations:\n",
    "\n",
    "- **Normalizer**: Lowercase, Unicode (NFD, NFKD, NFC, NFKC), Bert, Strip, ...\n",
    "- **PreTokenizer**: ByteLevel, WhitespaceSplit, CharDelimiterSplit, Metaspace, ...\n",
    "- **Model**: WordLevel, BPE, WordPiece\n",
    "- **Post-Processor**: BertProcessor, ...\n",
    "- **Decoder**: WordLevel, BPE, WordPiece, ...\n",
    "\n",
    "All of these building blocks can be combined to create working tokenization pipelines. \n",
    "In the next section we will go over our first pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Alright, now we are ready to implement our first tokenization pipeline through `tokenizers`. \n",
    "\n",
    "For this, we will train a Byte-Pair Encoding (BPE) tokenizer on a quite small input for the purpose of this notebook. You can train your tokenizer from a [Dataset](https://huggingface.co/docs/datasets/package_reference/main_classes.html#dataset) instance or from text files. We will see together the first method.\n",
    "\n",
    "For this example, we propose to use \"wikitext-2-raw-v1\" from [Wikitext dataset](https://huggingface.co/datasets/wikitext) which contains only 4.50 MB of text, but you can of course use a much larger dataset to train your tokenizer! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% code\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install tokenizers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% code\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wikitext (/home/lucile/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Initialize a dataset\n",
    "dataset = load_dataset(\"wikitext\", name=\"wikitext-2-raw-v1\", split=\"train\")\n",
    "\n",
    "# Build an iterator over this dataset\n",
    "def batch_iterator():\n",
    "    batch_length = 1000\n",
    "    for i in range(0, len(dataset), batch_length):\n",
    "        yield dataset[i : i + batch_length][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now that we have our training data we need to create the overall pipeline for the tokenizer. For this we have two options: \n",
    "- Solution 1: use one of the high-level classes encapsulating the overall pipeline for various well-known tokenization algorithm. The available tokenizers are listed [here](https://github.com/huggingface/tokenizers/tree/master/bindings/python/py_src/tokenizers/implementations).\n",
    "- Solution 2: if you want to do something more custom, you can define the tokenizer yourself.\n",
    "\n",
    "For both cases, the pipeline will be the same. It will be necessary first to instantiate a tokenizer architecture before training it.\n",
    "\n",
    "We will see here how to create a Byte-level BPE tokenizer as introduced by OpenAI with their GPT-2 model with the 2 methods and train them on the dataset loaded above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training parameter\n",
    "\n",
    "# the size of our tokens vocabulary\n",
    "vocab_size = 25000\n",
    "\n",
    "# the list of special tokens to have into our vocabulary. Several models use special tokens to mark the beginning of a sentence, the end of a sentence or to pad a sequence of tokens to a certain length. These special tokens must also belong to the vocabulary.\n",
    "special_tokens = [\"<|endoftext|>\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution 1: Use a tokenization pipeline already implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "# Initialize a not train tokenizer\n",
    "tokenizer_1 = ByteLevelBPETokenizer(unicode_normalizer=\"nfkc\", lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained vocab size: 25000\n"
     ]
    }
   ],
   "source": [
    "# Train it\n",
    "tokenizer_1.train_from_iterator(batch_iterator(), vocab_size=vocab_size, special_tokens=special_tokens)\n",
    "\n",
    "print(\"Trained vocab size: {}\".format(tokenizer.get_vocab_size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution 2: Define everything yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.normalizers import Lowercase, NFKC, Sequence\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "\n",
    "# First we create an empty Byte-Pair Encoding model (i.e. not trained model)\n",
    "tokenizer = Tokenizer(BPE())\n",
    "\n",
    "# Then we enable lower-casing and unicode-normalization\n",
    "# The Sequence normalizer allows us to combine multiple Normalizer that will be\n",
    "# executed in order.\n",
    "tokenizer.normalizer = Sequence([NFKC(), Lowercase()])\n",
    "\n",
    "# Our tokenizer also needs a pre-tokenizer responsible for converting the input to a ByteLevel representation.\n",
    "tokenizer.pre_tokenizer = ByteLevel()\n",
    "\n",
    "# And finally, let's plug a decoder so we can recover from a tokenized input to the original one\n",
    "tokenizer.decoder = ByteLevelDecoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% code\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained vocab size: 25000\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "# We initialize our trainer, giving him the details about the vocabulary we want to generate\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=vocab_size, show_progress=True, initial_alphabet=ByteLevel.alphabet(), special_tokens=special_tokens\n",
    ")\n",
    "tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)\n",
    "\n",
    "print(\"Trained vocab size: {}\".format(tokenizer.get_vocab_size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Et voilà ! You trained your very first tokenizer from scratch using `tokenizers`. Of course, this \n",
    "covers only the basics, and you may want to have a look at the documentation to customize your tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, let see how behave our newly trained tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% code\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded string: ['Ġthis', 'Ġis', 'Ġa', 'Ġsimple', 'Ġinput', 'Ġto', 'Ġbe', 'Ġto', 'ken', 'ized']\n",
      "Decoded string:  this is a simple input to be tokenized\n"
     ]
    }
   ],
   "source": [
    "# Let's tokenizer a simple input\n",
    "encoding = tokenizer.encode(\"This is a simple input to be tokenized\")\n",
    "\n",
    "print(\"Encoded string: {}\".format(encoding.tokens))\n",
    "\n",
    "decoded = tokenizer.decode(encoding.ids)\n",
    "print(\"Decoded string: {}\".format(decoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The Encoding structure exposes multiple properties which are useful when working with transformers models\n",
    "\n",
    "- normalized_str: The input string after normalization (lower-casing, unicode, stripping, etc.)\n",
    "- original_str: The input string as it was provided\n",
    "- tokens: The generated tokens with their string representation\n",
    "- input_ids: The generated tokens with their integer representation\n",
    "- attention_mask: If your input has been padded by the tokenizer, then this would be a vector of 1 for any non padded token and 0 for padded ones.\n",
    "- special_token_mask: If your input contains special tokens such as [CLS], [SEP], [MASK], [PAD], then this would be a vector with 1 in places where a special token has been added.\n",
    "- type_ids: If your input was made of multiple \"parts\" such as (question, context), then this would be a vector with for each token the segment it belongs to.\n",
    "- overflowing: If your input has been truncated into multiple subparts because of a length limit (for BERT for example the sequence length is limited to 512), this will contain all the remaining overflowing parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use your brand-new tokenizer into Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer_fast = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer_1,\n",
    "    unk_token=\"<|endoftext|>\",\n",
    "    bos_token=\"<|endoftext|>\",\n",
    "    eos_token=\"<|endoftext|>\",\n",
    "    add_prefix_space=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and save it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('brand-new-tokenizer/tokenizer_config.json',\n",
       " 'brand-new-tokenizer/special_tokens_map.json',\n",
       " 'brand-new-tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_fast.save_pretrained('brand-new-tokenizer')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
